\section{EXPERIMENTS AND RESULTS}\label{4_experiment}
We have run a number of experiments based on several common real world scenarios, and we compared our approach with other popular methods through the experimentation:
\begin{itemize}
    \item How effective MERec performs in $Pure$ $Cold$ recommendation tasks. i.e. new product release, target audience prediction.
    \item How effective When MERec handle $Sparse$ dataset, in common recommendation tasks
    \item How dose MERec compare with other state-of-the-art methods
    \item illustrate the adaptiveness of MERec 
\end{itemize}

\subsection{Data Set}
Our experiments is based on HetRec 2011 \cite{CantadorRecSys2011} data set, which is a extension of MovieLens10M data enriched with content data collected from IMDb and Rotten Tomatoes.
First, we prune the data. For movies, we only picks the movie with at least 10 distinctive user views. we also limit top 3 actors/actresses to be associated. In terms of Tags, we are only use tags information which being share by more than 1 movies. In the end, we concluded 6 different types of $node$ which can be used in heterogeneous information graph, shown as following:
% # users: 2113, # movies: 6829, # medium rating: 3.5, # ratings: 841910, #tags: 2356, # actors: 9295, # directors: 2762 # genres: 19
\begin{center}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
     \textbf{Node Type} & \textbf{Unique Number} \\ \hline
     Users &  2113\\ \hline
     Movies &   6829 \\ \hline
     Tags &  2356 \\ \hline
     Actors &  22256 \\ \hline
     Directors &  2762 \\ \hline
     Genres &  19 \\ \hline
    \end{tabular}
\end{center}

Additionally, following 5 relationships is defined as edges, which are $Movie-Users$, $Movie-Tags$, $Movie-Actors$, $Movie-Directors$, and $Movie-Genres$:

\begin{center}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
     \textbf{Edge Type} & \textbf{Total Number} \\ \hline
     $Movie-Users$ &  841910\\ \hline
     $Movie-Tags$ &   9362 \\ \hline
     $Movie-Actors$ &  2356 \\ \hline
     $Movie-Directors$ &  6893 \\ \hline
     $Movie-Genres$ &  15119 \\ \hline
    \end{tabular}
\end{center}

For the simplicity of the experiment, we weigh all of our meta-path equally, as it is sufficient in illustrating the effectiveness of MERec. On the other hand, we do acknowledge that, those hyper-parameters can be further tuned to improve the result to tailor different problem domains. 


\subsection{Pure cold start: MERec + Pearson Correlation Coefficient (PCC)}
When it comes to $Pure$ $Cold$ start problem, it means, new items are being recommended to users which had never seen before. as a result are are splitting our data set based on time.
we use movies prior 2008 as training data set (6724 movies), and 2008~2011 as testing data set (193 movies). Here we compare MERec with Content-Based (C.B.) recommendation, since CF approach will be very limited when there is no user-item interactions available.

The choice of meta-path in this experiment are: $Movie-Tags-Movie$, $Movie-Actors-Movie$, $Movie-Directors-Movie$, and $Movie-Genres-Movie$. We only choose feature related meta-path. By doing this way, we can not only can demonstrate MERec produces superior Precision and Recall. It also shows meta-path based movie content representation are more effective when dealing with large amount of categorical data. 
We also ran a separate experiment to add $Movie-Users-Movie$ meta-path as part of embedding process. we can see the result comparison as below:


    \textbf{pure cold start recommendation}
    \begin{center}
    \begin{tabular}{|c | c | c | c | c | c | c|} \hline

    \textbf{Model} & \textbf{Precision\@1} & \textbf{Recall@1} & \textbf{Precision@5} & \textbf{Recall@5} & \textbf{Precision@10} & \textbf{Recall@10} \\ \hline
    \text{C.B.} &  0.1390 & 0.0027 & 0.1229 & 0.0146 & 0.1481 & 0.0334 \\ \hline
    \text{MERec+PCC} & \textbf{0.3743} & 0.0087 & 0.2770 & 0.0350 & 0.2465 & 0.0604\\ \hline
    \text{MERec+PCC (incl. interactions)} & 0.34759 &  \textbf{0.0096} &  \textbf{0.3037} &  \textbf{0.0402} &  \textbf{0.2711} &  \textbf{0.0665} \\ \hline
    \end{tabular}
\end{center}

Here we can observe a further improved to the end result, after including past user-item interaction into the embedding process.


\subsection{Cold start with Sparse data: MERec + BPR}
In this section we are going to split data into 2 different density distribution to evaluate the effectiveness of MERec performance in both sparse and dense dataset. we also compares it with CF+BPR, which is being one of the most popular and effective algorithms.

First we split our dataset into 2 different random split portions: In sparse data scenarios, the data density set to be \text{1.1\%}, while in non-sparse experiment settings, the density is set at \text{2.3\%}

the comparison result are shown as flowing, In sparse data cases, we can see in average near 10\% performance boost in precision. While in non-sparse cases, we can still see equivalent or minor enhancement across precision and recall:

    \textbf{sparse data recommendation}
    \begin{center}
        \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
    
    \textbf{Model} & \textbf{Precision\@1} & \textbf{Recall@1} & \textbf{Precision@5} & \textbf{Recall@5} & \textbf{Precision@10} & \textbf{Recall@10} \\ \hline
         \text{CF+BPR (sparse)} &  0.6146 & \textbf{0.0039} & 0.5498 & 0.0126 & 0.5548 & 0.0265 \\ \hline
         \text{MERec+BPR (sparse)} & \textbf{0.6616} & 0.0036 & \textbf{0.6194} & \textbf{0.0163} & \textbf{0.6035} & \textbf{0.0308}\\ \hline
        \end{tabular}
    \end{center}
    



    \textbf{dense data recommendation}
    \begin{center}
        \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
    
    \textbf{Model} & \textbf{Precision\@1} & \textbf{Recall@1} & \textbf{Precision@5} & \textbf{Recall@5} & \textbf{Precision@10} & \textbf{Recall@10} \\ \hline
         \text{CF+BPR (dense)} & 0.4936 & 0.0027 & 0.5566 & 0.0175 & 0.5419 & 0.0336 \\ \hline
         \text{MERec+BPR (dense)} & \textbf{0.6720} & \textbf{0.0046} & \textbf{0.5858} & \textbf{0.0195} & \textbf{0.5575} & \textbf{0.0355} \\ \hline
        \end{tabular}
    \end{center}









